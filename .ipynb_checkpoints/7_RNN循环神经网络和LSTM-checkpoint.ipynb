{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LSTM实现了三个门计算，即遗忘门、输入门和输出门\n",
    " \n",
    " 遗忘门负责决定保留多少上一时刻的单元状态到当前时刻的单元状态；\n",
    " \n",
    " 输入门负责决定保留多少当前时刻的输入到当前时刻的单元状态；\n",
    " \n",
    " 输出门负责决定当前时刻的单元状态有多少输出。\n",
    " \n",
    " 算法核心在于记忆单元(过去记忆*遗忘门+现在输入*输入门)。会产生两种输出，一个是记忆单元c，一个是隐藏层的输出h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入数据集\n",
    "mnist=input_data.read_data_sets('MNIST_data',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=50#每个批次50个样本\n",
    "n_batch=mnist.train.num_examples//batch_size#数据集共有多少批次\n",
    "\n",
    "max_time=28#每个样本的长度(timestep)\n",
    "n_input=28#每个样本的维度\n",
    "lstm_size=100#隐藏单元数量为100\n",
    "n_class=10#10个分类\n",
    "\n",
    "#定义两个占位符，用于输入样本的值和标签\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#初始化权值和偏置\n",
    "weight=tf.Variable(tf.truncated_normal([lstm_size,n_class],stddev=0.1))\n",
    "bias=tf.Variable(tf.constant(0.1,shape=[n_class]))\n",
    "\n",
    "def RNN(x,weight,bias):\n",
    "    #dynamic_run函数的输入格式固定[样本数量，时间步(理解为每个样本长度，可改)，每个时间步维度]\n",
    "    inputs=tf.reshape(x,[-1,max_time,n_input])\n",
    "    #定义LSTM的cell\n",
    "    lstm_cell=tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "    #循环运行该神经网络，使用LSTM提高长期记忆；outputs表示返回每一次timestep返回值，final_state表示最后一个timestep返回的值\n",
    "    outputs,final_state=tf.nn.dynamic_rnn(lstm_cell,inputs,dtype=tf.float32)\n",
    "    #final_state[0]表示记忆单元c,final_state[1]表示隐藏层输出h\n",
    "    \n",
    "      #outputs:\n",
    "      # If time_major == False (default), this will be a `Tensor` shaped:\n",
    "      #   `[batch_size, max_time, cell.output_size]`.\n",
    "      # If time_major == True, this will be a `Tensor` shaped:\n",
    "      #   `[max_time, batch_size, cell.output_size]`.\n",
    "        \n",
    "    result=tf.nn.softmax(tf.matmul(final_state[1],weight)+bias)\n",
    "    return  result\n",
    "\n",
    "#带入数据后，RNN返回结果\n",
    "prediction=RNN(x,weight,bias)\n",
    "# 交叉熵代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=prediction))\n",
    "# 使用AdamOptimizer优化，使得loss最小\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "# 初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 比较概率最大的标签是否相同，结果存放在一个布尔型列表中\n",
    "correct_predict = tf.equal(tf.argmax(y, 1), tf.argmax(prediction, 1))  # argmax返回一维张量中最大值所在的位置\n",
    "# 求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))  # reduce_mean求平均值\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(8):  # 可优化\n",
    "        for batch in range(n_batch):  # 把所有图片训练一次\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            \n",
    "        # 用测试数据来检验训练好的模型\n",
    "        acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Iter \" + str(epoch) + \"Test accuracy\" + str(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
